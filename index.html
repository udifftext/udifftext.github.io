<!DOCTYPE html>
<html>


<head>

  <meta charset="utf-8">
  <meta name="description" content="A Unified Framework for High-quality Text Synthesis in Arbitrary Images via Character-aware Diffusion Models.">
  <meta name="keywords" content="Diffusion, Stable Diffusion, SD, Scene text editing, STE">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>UDiffText | Home</title>

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro">
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

</head>


<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">

          <h1 class="title is-1 publication-title">UDiffText</h1>
          <h2 class="title is-3 publication-title">A Unified Framework for High-quality Text Synthesis in Arbitrary Images via Character-aware Diffusion Models</h2>

          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://github.com/ZYM-PKU">Yiming Zhao</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="">Zhouhui Lian</a></span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">Peking University</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/ZYM-PKU/UDiffText"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Demo Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Demo</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay controls muted loop playsinline width="100%">
        <source src="./static/videos/demo video.mp4" type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">UDiffText</span> can synthesize the text you want in arbitrary images
      </h2>
    </div>
  </div>
</section>


<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item">
          <video poster="123" id="0" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/0.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" id="1" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/1.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" id="2" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/2.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" id="3" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/3.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" id="4" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/4.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" id="5" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/5.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" id="6" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/6.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" id="7" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/7.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" id="8" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/8.mp4" type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section is-light is-small">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            we aim to design a unified framework for high-quality text synthesis in both synthetic and real-world images. 
            The proposed method, UDiffText, is built based on the inpainting variant of Stable Diffusion (v2.0). 
            Specifically, we first design and train a light-weight character-level (CL) text encoder as a substitute for the original CLIP text encoder. 
            Then, we train the model using the denoising score matching (DSM) loss in conjunction with the local attention loss and scene text recognition loss. 
          </p>
          <div class="column is-centered has-text-centered">
            <img src="static/images/teaser.pdf" alt="structure"/>
          </div>
          <p>
            An overview of the training process of our proposed UDiffText. 
            We build our model based on the inpainting version of Stable Diffusion (v2.0). 
            A character-level (CL) text encoder is utilized to obtain robust embeddings from the text to be rendered. 
            We train the model using denoising score matching (DSM) together with the local attention loss calculated based on 
            character-level segmentation maps and the auxiliary scene text recognition loss. 
            Note that only the parameters of cross-attention (CA) blocks are updated during training.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div> -->
    <!--/ Paper video. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered"></div>
      <video poster="" id="tree" autoplay muted loop playsinline width="100%"><source src="static/videos/illu1.mp4" type="video/mp4"></video>
      <video poster="" id="tree" autoplay muted loop playsinline width="100%"><source src="static/videos/illu2.mp4" type="video/mp4"></video>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method</h2>
        <div class="content has-text-justified">
          <p>
            Text-to-Image (T2I) generation methods based on diffusion model have garnered significant attention in the last few years. 
            Although these image synthesis methods produce visually appealing results, they frequently exhibit spelling errors when rendering text within the generated images. 
            Such errors manifest as missing, incorrect or extraneous characters, thereby severely constraining the performance of text image generation based on diffusion models. 
            To address the aforementioned issue, this paper proposes a novel approach for text image generation, utilizing a pre-trained diffusion model (i.e., Stable Diffusion). 
          </p>
          <p>
            Our approach involves the design and training of a light-weight character-level text encoder, 
            which replaces the original CLIP encoder and provides more robust text embeddings as conditional guidance. 
            Then, we  fine-tune the diffusion model using a large-scale dataset, incorporating local attention control under the supervision of character-level segmentation maps. 
            Finally, by employing an inference stage refinement process, we achieve a notably high sequence accuracy when synthesizing text in arbitrarily given images. 
            Both qualitative and quantitative results demonstrate the superiority of our method to the state of the art. 
            Furthermore, we showcase several potential applications of the proposed UDiffText, including text-centric image synthesis, scene text editing, etc. 
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div> -->
    <!--/ Paper video. -->
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code> Coming soon
    <!-- @article{park2021nerfies,
      author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
      title     = {Nerfies: Deformable Neural Radiance Fields},
      journal   = {ICCV},
      year      = {2021},
    } -->
    </code></pre>
  </div>
</section>


<footer class="footer">
  <div class="columns is-centered">
    <div class="column is-8">
      <div class="content">
        <p>
          This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
        </p>
        <p>
          Website source code based on the <a href="https://nerfies.github.io/"> Nerfies</a> project page. 
          If you want to reuse their <a href="https://github.com/nerfies/nerfies.github.io">source code</a>, please credit them appropriately.
        </p>
      </div>
    </div>
  </div>
</footer>


</body>
</html>
